{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Progress Prediction (XGBoost)\n",
    "Notebook này **chỉ dùng XGBoost** để:\n",
    "- Tạo dữ liệu feature-engineering (đồng bộ với `pipeline.py`).\n",
    "- Train lại model trên toàn bộ dữ liệu.\n",
    "- Sinh file dự đoán `submissionn.csv` từ `data/test.csv`.\n",
    "\n",
    "Gợi ý workflow: chạy notebook này xong rồi mới chạy Streamlit app (`streamlit_app.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import & cấu hình\n",
    "- Chạy notebook tại thư mục gốc project (nơi có `data/`, `pipeline.py`).\n",
    "- Nếu chạy trên máy không có GPU/CUDA, cứ dùng CPU bình thường."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "try:\n",
    "    import optuna  # optional nhưng khuyến nghị để tune\n",
    "except Exception:\n",
    "    optuna = None\n",
    "\n",
    "# ============================\n",
    "# Pipeline inline (no imports)\n",
    "# ============================\n",
    "FEATURES: List[str] = [\n",
    "    \"TC_DANGKY\",\n",
    "    \"SO_NAM_HOC\",\n",
    "    \"COVID_ONLINE_THPT\",\n",
    "    \"COVID_ONLINE_DH\",\n",
    "    \"HIST_AVG_GPA\",\n",
    "    \"HIST_AVG_TC_DANGKY\",\n",
    "    \"HIST_PASS_RATE\",\n",
    "    \"HIST_GPA_STD\",\n",
    "    \"TOTAL_FAIL_CREDITS\",\n",
    "    \"LAG1_GPA\",\n",
    "    \"LAG2_GPA\",\n",
    "    \"LAG1_PASS_RATE\",\n",
    "    \"TREND_GPA\",\n",
    "    \"LOAD_RATIO\",\n",
    "    \"SCORE_GAP\",\n",
    "    \"DIEM_TRUNGTUYEN\",\n",
    "    \"DIEM_CHUAN\",\n",
    "    \"NAM_TUYENSINH\",\n",
    "    \"PTXT_1\",\n",
    "    \"PTXT_100\",\n",
    "    \"PTXT_200\",\n",
    "    \"PTXT_3\",\n",
    "    \"PTXT_402\",\n",
    "    \"PTXT_409\",\n",
    "    \"PTXT_5\",\n",
    "    \"PTXT_500\",\n",
    "    \"AVG_SCORE\",\n",
    "]\n",
    "\n",
    "def _get_current_year(hoc_ky: str) -> int:\n",
    "    arr = str(hoc_ky).split(\"-\")\n",
    "    return int(arr[-1])\n",
    "\n",
    "def _key_hoc_ky(hoc_ky: str) -> Tuple[int, int]:\n",
    "    # Format: 'HK1 2020-2021'\n",
    "    hk, year = str(hoc_ky).split()\n",
    "    hk = int(hk.replace(\"HK\", \"\"))\n",
    "    year = int(year.split(\"-\")[0])\n",
    "    return (year, hk)\n",
    "\n",
    "def _latest_semester_label(values: Iterable[str]) -> Optional[str]:\n",
    "    labels = [v for v in pd.Series(list(values)).dropna().astype(str).unique().tolist()]\n",
    "    if not labels:\n",
    "        return None\n",
    "    labels_sorted = sorted(labels, key=_key_hoc_ky)\n",
    "    return labels_sorted[-1]\n",
    "\n",
    "def build_main_df(academic_df: pd.DataFrame, admission_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rebuild the engineered dataset.\n",
    "\n",
    "    Output contains one row per (MA_SO_SV, HOC_KY) with engineered history features.\n",
    "    \"\"\"\n",
    "    df = pd.merge(academic_df, admission_df, how=\"left\", on=\"MA_SO_SV\")\n",
    "\n",
    "    # One-hot PTXT (keep stable set as much as possible)\n",
    "    if \"PTXT\" in df.columns:\n",
    "        dummies = pd.get_dummies(df[\"PTXT\"], prefix=\"PTXT\", dtype=float)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df = df.drop(columns=[\"PTXT\"])\n",
    "\n",
    "    df[\"SO_NAM_HOC\"] = df[\"HOC_KY\"].apply(_get_current_year) - df[\"NAM_TUYENSINH\"]\n",
    "\n",
    "    df[\"_key_hocky\"] = df[\"HOC_KY\"].map(_key_hoc_ky)\n",
    "    df = df.sort_values(by=[\"MA_SO_SV\", \"_key_hocky\"]).drop(columns=[\"_key_hocky\"])\n",
    "\n",
    "    # History aggregates\n",
    "    df[\"HIST_AVG_GPA\"] = (\n",
    "        df.groupby(\"MA_SO_SV\")[\"GPA\"].transform(lambda x: x.shift(1).expanding().mean()).fillna(0)\n",
    "    )\n",
    "    df[\"HIST_AVG_TC_DANGKY\"] = (\n",
    "        df.groupby(\"MA_SO_SV\")[\"TC_DANGKY\"]\n",
    "        .transform(lambda x: x.shift(1).expanding().mean())\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    df[\"HIST_PASS_RATE\"] = (\n",
    "        df.groupby(\"MA_SO_SV\")[\"TC_HOANTHANH\"].cumsum().shift(1)\n",
    "        / df.groupby(\"MA_SO_SV\")[\"TC_DANGKY\"].cumsum().shift(1)\n",
    "    ).fillna(1)\n",
    "\n",
    "    df[\"LAG1_GPA\"] = df.groupby(\"MA_SO_SV\")[\"GPA\"].shift(1).fillna(0)\n",
    "    df[\"LAG2_GPA\"] = df.groupby(\"MA_SO_SV\")[\"GPA\"].shift(2).fillna(0)\n",
    "    df[\"LAG1_PASS_RATE\"] = df.groupby(\"MA_SO_SV\")[\"HIST_PASS_RATE\"].shift(1).fillna(1)\n",
    "    df[\"TREND_GPA\"] = (df[\"LAG1_GPA\"] - df[\"HIST_AVG_GPA\"]).fillna(0)\n",
    "\n",
    "    df[\"PASS_RATE\"] = (df[\"TC_HOANTHANH\"] / df[\"TC_DANGKY\"]).replace([np.inf, -np.inf], np.nan)\n",
    "    df[\"PASS_RATE\"] = df[\"PASS_RATE\"].fillna(0).astype(float)\n",
    "\n",
    "    df[\"SCORE_GAP\"] = df[\"DIEM_TRUNGTUYEN\"] - df[\"DIEM_CHUAN\"]\n",
    "\n",
    "    df[\"HIST_GPA_STD\"] = (\n",
    "        df.groupby(\"MA_SO_SV\")[\"GPA\"].transform(lambda x: x.shift(1).expanding().std()).fillna(0)\n",
    "    )\n",
    "\n",
    "    df[\"FAIL_CREDITS_TEMP\"] = df[\"TC_DANGKY\"] - df[\"TC_HOANTHANH\"]\n",
    "    df[\"TOTAL_FAIL_CREDITS\"] = (\n",
    "        df.groupby(\"MA_SO_SV\")[\"FAIL_CREDITS_TEMP\"].transform(lambda x: x.shift(1).cumsum()).fillna(0)\n",
    "    )\n",
    "    df = df.drop(columns=[\"FAIL_CREDITS_TEMP\"])\n",
    "\n",
    "    df[\"LOAD_RATIO\"] = df[\"TC_DANGKY\"] / df[\"HIST_AVG_TC_DANGKY\"].replace(0, 1)\n",
    "\n",
    "    avg_score_map = {\n",
    "        2025: 6.50,\n",
    "        2024: 6.57,\n",
    "        2023: 6.03,\n",
    "        2022: 6.34,\n",
    "        2021: 4.97,\n",
    "        2020: 5.19,\n",
    "        2019: 4.30,\n",
    "        2018: 3.79,\n",
    "        2017: 4.59,\n",
    "        2016: 4.50,\n",
    "        2015: 5.25,\n",
    "    }\n",
    "    df[\"AVG_SCORE\"] = df[\"NAM_TUYENSINH\"].map(avg_score_map)\n",
    "\n",
    "    covid_years_thpt = [2020, 2021]\n",
    "    covid_semesters_dh = [\n",
    "        \"HK1 2019-2020\",\n",
    "        \"HK2 2019-2020\",\n",
    "        \"HK1 2020-2021\",\n",
    "        \"HK2 2020-2021\",\n",
    "        \"HK1 2021-2022\",\n",
    "    ]\n",
    "    df[\"COVID_ONLINE_THPT\"] = df[\"NAM_TUYENSINH\"].isin(covid_years_thpt).astype(int)\n",
    "    df[\"COVID_ONLINE_DH\"] = df[\"HOC_KY\"].isin(covid_semesters_dh).astype(int)\n",
    "\n",
    "    # Ensure any missing PTXT dummy columns exist for downstream.\n",
    "    for col in [c for c in FEATURES if c.startswith(\"PTXT_\")]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "\n",
    "    # Fill other feature columns if missing\n",
    "    for col in FEATURES:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_prediction_frame(\n",
    "    uploaded_df: pd.DataFrame,\n",
    "    main_df: pd.DataFrame,\n",
    "    preferred_last_hk: str = \"HK2 2023-2024\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Build (ready_df, pred_df) aligned to FEATURES.\"\"\"\n",
    "    if \"MA_SO_SV\" not in uploaded_df.columns or \"TC_DANGKY\" not in uploaded_df.columns:\n",
    "        raise ValueError(\"CSV upload phải có cột MA_SO_SV và TC_DANGKY.\")\n",
    "\n",
    "    last_hk = preferred_last_hk\n",
    "    if last_hk not in set(main_df[\"HOC_KY\"].astype(str).unique().tolist()):\n",
    "        inferred = _latest_semester_label(main_df[\"HOC_KY\"].astype(str).values)\n",
    "        if inferred is None:\n",
    "            raise ValueError(\"Không xác định được học kỳ gần nhất từ dữ liệu lịch sử.\")\n",
    "        last_hk = inferred\n",
    "\n",
    "    last_snapshot = main_df[main_df[\"HOC_KY\"].astype(str) == str(last_hk)].copy()\n",
    "\n",
    "    # Merge: uploaded test-like data + last snapshot\n",
    "    pred_df = pd.merge(uploaded_df, last_snapshot, how=\"left\", on=\"MA_SO_SV\", suffixes=(\"_x\", \"_y\"))\n",
    "\n",
    "    # Update SO_NAM_HOC for next term\n",
    "    if \"SO_NAM_HOC\" in pred_df.columns:\n",
    "        pred_df[\"SO_NAM_HOC\"] = pd.to_numeric(pred_df[\"SO_NAM_HOC\"], errors=\"coerce\").fillna(0) + 1\n",
    "    else:\n",
    "        pred_df[\"SO_NAM_HOC\"] = 0\n",
    "\n",
    "    pred_features = [\n",
    "        \"TC_DANGKY_x\",\n",
    "        \"SO_NAM_HOC\",\n",
    "        \"COVID_ONLINE_THPT\",\n",
    "        \"COVID_ONLINE_DH\",\n",
    "        \"HIST_AVG_GPA\",\n",
    "        \"HIST_AVG_TC_DANGKY\",\n",
    "        \"HIST_PASS_RATE\",\n",
    "        \"HIST_GPA_STD\",\n",
    "        \"TOTAL_FAIL_CREDITS\",\n",
    "        \"LAG1_GPA\",\n",
    "        \"LAG2_GPA\",\n",
    "        \"LAG1_PASS_RATE\",\n",
    "        \"TREND_GPA\",\n",
    "        \"LOAD_RATIO\",\n",
    "        \"SCORE_GAP\",\n",
    "        \"DIEM_TRUNGTUYEN\",\n",
    "        \"DIEM_CHUAN\",\n",
    "        \"NAM_TUYENSINH\",\n",
    "        \"PTXT_1\",\n",
    "        \"PTXT_100\",\n",
    "        \"PTXT_200\",\n",
    "        \"PTXT_3\",\n",
    "        \"PTXT_402\",\n",
    "        \"PTXT_409\",\n",
    "        \"PTXT_5\",\n",
    "        \"PTXT_500\",\n",
    "        \"AVG_SCORE\",\n",
    "    ]\n",
    "\n",
    "    for col in pred_features:\n",
    "        if col not in pred_df.columns:\n",
    "            pred_df[col] = 0\n",
    "\n",
    "    ready_df = pred_df[pred_features].rename(columns={\"TC_DANGKY_x\": \"TC_DANGKY\"})\n",
    "\n",
    "    # Align to FEATURES\n",
    "    ready_df = ready_df.reindex(columns=FEATURES, fill_value=0)\n",
    "\n",
    "    # Numeric safety\n",
    "    for col in FEATURES:\n",
    "        ready_df[col] = pd.to_numeric(ready_df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    return ready_df, pred_df\n",
    "\n",
    "def predict_credits(\n",
    "    model: XGBRegressor,\n",
    "    ready_df: pd.DataFrame,\n",
    "    tc_dangky: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    pass_rate = model.predict(ready_df)\n",
    "    pass_rate = np.clip(pass_rate, 0.0, 1.0)\n",
    "    tc = np.asarray(tc_dangky, dtype=float)\n",
    "    tc = np.nan_to_num(tc, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    pred_tc = tc * pass_rate\n",
    "    return pass_rate, pred_tc\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "BEST_PARAMS_PATH = Path(\"best_params_xgboost.json\")\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load dữ liệu (local)\n",
    "Cần đủ 3 file:\n",
    "- `data/academic_records.csv`\n",
    "- `data/admission.csv`\n",
    "- `data/test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "academic_path = DATA_DIR / 'academic_records.csv'\n",
    "admission_path = DATA_DIR / 'admission.csv'\n",
    "test_path = DATA_DIR / 'test.csv'\n",
    "\n",
    "for p in [academic_path, admission_path, test_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f'Missing file: {p}')\n",
    "\n",
    "academic_df = pd.read_csv(academic_path)\n",
    "admission_df = pd.read_csv(admission_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print('academic_df:', academic_df.shape)\n",
    "print('admission_df:', admission_df.shape)\n",
    "print('test_df:', test_df.shape)\n",
    "\n",
    "display(academic_df.head())\n",
    "display(admission_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature engineering (đồng bộ `pipeline.py`)\n",
    "Mục tiêu: tạo `main_df` gồm các feature lịch sử cho từng sinh viên theo từng học kỳ.\n",
    "Trong project này, phần feature engineering **được chuẩn hoá trong** `pipeline.build_main_df()` để tránh lệch logic giữa notebook và app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = build_main_df(academic_df, admission_df)\n",
    "print('main_df:', main_df.shape)\n",
    "\n",
    "# Sanity checks\n",
    "missing = [c for c in FEATURES if c not in main_df.columns]\n",
    "print('Missing feature cols:', missing)\n",
    "\n",
    "display(main_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Chuẩn bị train/valid\n",
    "Target đang train là `PASS_RATE` (tỷ lệ hoàn thành).\n",
    "Khi suy ra số tín chỉ hoàn thành, ta dùng: `PRED_TC_HOANTHANH = TC_DANGKY * PASS_RATE`.\n",
    "\n",
    "Để đánh giá hợp lý theo thời gian, ta giữ lại **1 học kỳ** làm validation (mặc định `HK2 2023-2024`). Nếu học kỳ này không tồn tại trong dữ liệu thì notebook sẽ tự lấy học kỳ mới nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key_hoc_ky(hoc_ky: str):\n",
    "    # Format: 'HK1 2020-2021'\n",
    "    hk, year = str(hoc_ky).split()\n",
    "    hk = int(hk.replace('HK', ''))\n",
    "    year = int(year.split('-')[0])\n",
    "    return (year, hk)\n",
    "\n",
    "preferred_valid_hk = os.environ.get('VALID_HK', 'HK2 2023-2024')\n",
    "hoc_ky_values = main_df['HOC_KY'].astype(str).dropna().unique().tolist()\n",
    "\n",
    "if preferred_valid_hk in hoc_ky_values:\n",
    "    valid_hk = preferred_valid_hk\n",
    "else:\n",
    "    valid_hk = sorted(hoc_ky_values, key=_key_hoc_ky)[-1]\n",
    "\n",
    "print('Using validation semester:', valid_hk)\n",
    "\n",
    "X = main_df[FEATURES].copy()\n",
    "y = main_df['PASS_RATE'].astype(float).copy()\n",
    "\n",
    "valid_mask = main_df['HOC_KY'].astype(str) == str(valid_hk)\n",
    "X_train, y_train = X.loc[~valid_mask], y.loc[~valid_mask]\n",
    "X_valid, y_valid = X.loc[valid_mask], y.loc[valid_mask]\n",
    "\n",
    "print('Train:', X_train.shape, 'Valid:', X_valid.shape)\n",
    "print('PASS_RATE train mean:', float(y_train.mean()), 'valid mean:', float(y_valid.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train baseline XGBoost (không tune)\n",
    "Cell này giúp kiểm tra nhanh pipeline có chạy ổn không trước khi tune Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_params = {\n",
    "    'n_estimators': 800,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'min_child_weight': 2,\n",
    "    'reg_lambda': 1.0,\n",
    "    'reg_alpha': 0.0,\n",
    "    'gamma': 0.0,\n",
    "    'tree_method': 'hist',  # CPU friendly\n",
    "}\n",
    "\n",
    "baseline_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **baseline_params,\n",
    ")\n",
    "\n",
    "baseline_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "pred_valid = baseline_model.predict(X_valid)\n",
    "print('Baseline RMSE:', rmse(y_valid, pred_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) (Tuỳ chọn) Tune XGBoost bằng Optuna\n",
    "- Mặc định chạy nhanh với `N_TRIALS=30`. Có thể tăng lên 100-300 để tối ưu kỹ hơn.\n",
    "- Nếu có GPU và XGBoost support CUDA, set biến môi trường `USE_CUDA=1` trước khi chạy kernel.\n",
    "- Kết quả sẽ được lưu ra `best_params_xgboost.json` để Streamlit app dùng lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = int(os.environ.get('N_TRIALS', '30'))\n",
    "USE_CUDA = os.environ.get('USE_CUDA', '0') == '1'\n",
    "\n",
    "if optuna is None:\n",
    "    raise ImportError('Optuna chưa được cài. Hãy chạy: pip install optuna (hoặc pip install -r requirements.txt)')\n",
    "\n",
    "def suggest_xgb_params(trial: optuna.trial.Trial) -> dict:\n",
    "    params = {\n",
    "        'tree_method': 'hist',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 300, 4000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.95),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-6, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 10.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "    }\n",
    "    if USE_CUDA:\n",
    "        # XGBoost >= 2.0\n",
    "        params['device'] = 'cuda'\n",
    "    return params\n",
    "\n",
    "def objective(trial) -> float:\n",
    "    params = suggest_xgb_params(trial)\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_valid)\n",
    "    return rmse(y_valid, pred)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "print('[Optuna] Best RMSE:', study.best_value)\n",
    "print('[Optuna] Best params:', study.best_params)\n",
    "\n",
    "# Lưu best params để Streamlit/app dùng lại\n",
    "with open(BEST_PARAMS_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(study.best_params, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Saved:', BEST_PARAMS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Train final model trên toàn bộ dữ liệu\n",
    "Cell này đọc `best_params_xgboost.json` (nếu có) rồi fit trên toàn bộ `main_df`.\n",
    "Streamlit app cũng làm tương tự, nên bước này chủ yếu để bạn kiểm tra nhanh output ở notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "if BEST_PARAMS_PATH.exists():\n",
    "    best_params = json.loads(BEST_PARAMS_PATH.read_text(encoding='utf-8'))\n",
    "\n",
    "final_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "final_model.fit(X, y)\n",
    "print('Final model fitted. best_params keys:', list(best_params.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Dự báo cho `data/test.csv` và xuất `submissionn.csv`\n",
    "Logic tạo feature cho prediction được chuẩn hoá trong `pipeline.build_prediction_frame()` để đồng bộ với Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_df, pred_df = build_prediction_frame(test_df, main_df, preferred_last_hk=os.environ.get('LAST_HK', 'HK2 2023-2024'))\n",
    "\n",
    "tc_dangky = pred_df.get('TC_DANGKY_x', pred_df.get('TC_DANGKY', 0)).values\n",
    "pass_rate, pred_tc = predict_credits(final_model, ready_df, tc_dangky=tc_dangky)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    'MA_SO_SV': pred_df['MA_SO_SV'],\n",
    "    'PRED_PASS_RATE': pass_rate,\n",
    "    'PRED_TC_HOANTHANH': pred_tc,\n",
    "})\n",
    "\n",
    "out_path = 'submissionn.csv'\n",
    "out.to_csv(out_path, index=False)\n",
    "print('Saved:', out_path, 'shape:', out.shape)\n",
    "display(out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Bước tiếp theo\n",
    "Sau khi đã có `best_params_xgboost.json`, bạn chạy app:\n",
    "```bash\n",
    "streamlit run streamlit_app.py\n",
    "```\n",
    "App sẽ tự load lại dữ liệu + train model theo params và sinh SHAP note cho từng dòng dự đoán."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9364197,
     "sourceId": 14658403,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
